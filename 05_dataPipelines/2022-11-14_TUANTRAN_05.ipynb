{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250ce160",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Solution ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c209aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import time,math\n",
    "\n",
    "# This limits the amount of memory used:\n",
    "os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"] = \"true\"\n",
    "os.environ[\"TF_XLA_FLAGS\"] = \"--tf_xla_auto_jit=2\"\n",
    "# This control parallelism in Tensorflow\n",
    "parallel_threads = 128\n",
    "# This controls how many batches to prefetch\n",
    "prefetch_buffer_size = 8 # tf.data.AUTOTUNE\n",
    "os.environ['OMP_NUM_THREADS'] = str(parallel_threads)\n",
    "num_parallel_readers = parallel_threads\n",
    "\n",
    "# how many training steps to take during profiling\n",
    "num_steps = 10\n",
    "use_profiler = True\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.profiler import trace\n",
    "\n",
    "#########################################################################\n",
    "# Here's the Residual layer from the first half again:\n",
    "#########################################################################\n",
    "class ResidualLayer(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, n_filters):\n",
    "        # tf.keras.Model.__init__(self)\n",
    "        super(ResidualLayer, self).__init__()\n",
    "\n",
    "        self.conv1 = tf.keras.layers.Conv2D(\n",
    "            filters     = n_filters,\n",
    "            kernel_size = (3,3),\n",
    "            padding     = \"same\"\n",
    "        )\n",
    "\n",
    "        self.norm1 = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "        self.conv2 = tf.keras.layers.Conv2D(\n",
    "            filters     = n_filters,\n",
    "            kernel_size = (3,3),\n",
    "            padding     = \"same\"\n",
    "        )\n",
    "\n",
    "        self.norm2 = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "    def call(self, inputs):\n",
    "\n",
    "        x = inputs\n",
    "\n",
    "        output1 = self.norm1(self.conv1(inputs))\n",
    "\n",
    "        output1 = tf.keras.activations.relu(output1)\n",
    "\n",
    "        output2 = self.norm2(self.conv2(output1))\n",
    "\n",
    "        return tf.keras.activations.relu(output2 + x)\n",
    "\n",
    "#########################################################################\n",
    "# Here's layer that does a spatial downsampling:\n",
    "#########################################################################\n",
    "class ResidualDownsample(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, n_filters):\n",
    "        # tf.keras.Model.__init__(self)\n",
    "        super(ResidualDownsample, self).__init__()\n",
    "\n",
    "        self.conv1 = tf.keras.layers.Conv2D(\n",
    "            filters     = n_filters,\n",
    "            kernel_size = (3,3),\n",
    "            padding     = \"same\",\n",
    "            strides     = (2,2)\n",
    "        )\n",
    "\n",
    "        self.identity = tf.keras.layers.Conv2D(\n",
    "            filters     = n_filters,\n",
    "            kernel_size = (1,1),\n",
    "            strides     = (2,2),\n",
    "            padding     = \"same\"\n",
    "        )\n",
    "\n",
    "        self.norm1 = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "        self.conv2 = tf.keras.layers.Conv2D(\n",
    "            filters     = n_filters,\n",
    "            kernel_size = (3,3),\n",
    "            padding     = \"same\"\n",
    "        )\n",
    "\n",
    "        self.norm2 = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "\n",
    "        x = self.identity(inputs)\n",
    "        output1 = self.norm1(self.conv1(inputs))\n",
    "        output1 = tf.keras.activations.relu(output1)\n",
    "\n",
    "        output2 = self.norm2(self.conv2(output1))\n",
    "\n",
    "        return tf.keras.activations.relu(output2 + x)\n",
    "\n",
    "\n",
    "#########################################################################\n",
    "# Armed with that, let's build ResNet (this particular one is called ResNet34)\n",
    "#########################################################################\n",
    "\n",
    "class ResNet34(tf.keras.Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ResNet34, self).__init__()\n",
    "\n",
    "        self.conv_init = tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2D(\n",
    "                filters     = 64,\n",
    "                kernel_size = (7,7),\n",
    "                strides     = (2,2),\n",
    "                padding     = \"same\",\n",
    "                use_bias    = False\n",
    "            ),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2), padding=\"same\")\n",
    "\n",
    "        ])\n",
    "\n",
    "        self.residual_series_1 = tf.keras.Sequential([\n",
    "            ResidualLayer(64),\n",
    "            ResidualLayer(64),\n",
    "            ResidualLayer(64),\n",
    "        ])\n",
    "\n",
    "        # Increase the number of filters:\n",
    "        self.downsample_1 = ResidualDownsample(128)\n",
    "\n",
    "        self.residual_series_2 = tf.keras.Sequential([\n",
    "            ResidualLayer(128),\n",
    "            ResidualLayer(128),\n",
    "            ResidualLayer(128),\n",
    "        ])\n",
    "\n",
    "        # Increase the number of filters:\n",
    "        self.downsample_2 = ResidualDownsample(256)\n",
    "\n",
    "        self.residual_series_3 = tf.keras.Sequential([\n",
    "            ResidualLayer(256),\n",
    "            ResidualLayer(256),\n",
    "            ResidualLayer(256),\n",
    "            ResidualLayer(256),\n",
    "            ResidualLayer(256),\n",
    "        ])\n",
    "\n",
    "        # Increase the number of filters:\n",
    "        self.downsample_3 = ResidualDownsample(512)\n",
    "\n",
    "\n",
    "        self.residual_series_4 = tf.keras.Sequential([\n",
    "            ResidualLayer(512),\n",
    "            ResidualLayer(512),\n",
    "        ])\n",
    "\n",
    "        self.final_pool = tf.keras.layers.AveragePooling2D(\n",
    "            pool_size=(8,8)\n",
    "        )\n",
    "\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.classifier = tf.keras.layers.Dense(1000)\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "\n",
    "        x = self.conv_init(inputs)\n",
    "\n",
    "        x = self.residual_series_1(x)\n",
    "\n",
    "\n",
    "        x = self.downsample_1(x)\n",
    "\n",
    "\n",
    "        x = self.residual_series_2(x)\n",
    "\n",
    "        x = self.downsample_2(x)\n",
    "\n",
    "        x = self.residual_series_3(x)\n",
    "\n",
    "        x = self.downsample_3(x)\n",
    "\n",
    "\n",
    "        x = self.residual_series_4(x)\n",
    "\n",
    "\n",
    "        x = self.final_pool(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        logits = self.classifier(x)\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@tf.function()\n",
    "def calculate_accuracy(logits, labels):\n",
    "    # We calculate top1 accuracy only here:\n",
    "    selected_class = tf.argmax(logits, axis=1)\n",
    "\n",
    "    correct = tf.cast(selected_class, tf.float32) == tf.cast(labels, tf.float32)\n",
    "\n",
    "    return tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "\n",
    "@tf.function()\n",
    "def calculate_loss(logits, labels):\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels, logits)\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "@tf.function()\n",
    "def training_step(network, optimizer, images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = network(images)\n",
    "        loss = calculate_loss(logits, labels)\n",
    "\n",
    "    gradients = tape.gradient(loss, network.trainable_variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, network.trainable_variables))\n",
    "\n",
    "    accuracy = calculate_accuracy(logits, labels)\n",
    "\n",
    "    return loss, accuracy\n",
    "\n",
    "@trace.trace_wrapper('train_epoch')\n",
    "def train_epoch(i_epoch, step_in_epoch, train_ds, val_ds, network, optimizer, BATCH_SIZE, checkpoint):\n",
    "    # Here is our training loop!\n",
    "\n",
    "    steps_per_epoch = int(1281167 / BATCH_SIZE)\n",
    "    steps_validation = int(50000 / BATCH_SIZE)\n",
    "\n",
    "    # added for profiling\n",
    "    if use_profiler:\n",
    "        print('start profiler')\n",
    "        tf.profiler.experimental.start('logdir/m%03d_w%02d_p%02d' % (parallel_threads,num_parallel_readers,prefetch_buffer_size))\n",
    "    \n",
    "    start = time.time()\n",
    "    i = 0\n",
    "    sum = 0.\n",
    "    sum2 = 0.\n",
    "    for train_images, train_labels in train_ds.take(steps_per_epoch):\n",
    "        if step_in_epoch > steps_per_epoch: break\n",
    "        else: step_in_epoch.assign_add(1)\n",
    "\n",
    "        # Peform the training step for this batch\n",
    "        loss, acc = training_step(network, optimizer, train_images, train_labels)\n",
    "        end = time.time()\n",
    "        images_per_second = BATCH_SIZE / (end - start)\n",
    "        if i > 0: # skip the first measurement because it includes compile time\n",
    "            sum += images_per_second\n",
    "            sum2 += images_per_second * images_per_second\n",
    "        print(f\"Finished step {step_in_epoch.numpy()} of {steps_per_epoch} in epoch {i_epoch.numpy()},loss={loss:.3f}, acc={acc:.3f} ({images_per_second:.3f} img/s).\")\n",
    "        start = time.time()\n",
    "        # added for profiling to stop after some steps\n",
    "        i += 1\n",
    "        if i > num_steps and use_profiler: break\n",
    "    \n",
    "    # added for profiling to stop after some steps\n",
    "    if use_profiler:\n",
    "        print('stop profiler')\n",
    "        i = i - 1\n",
    "        mean_rate = sum / i\n",
    "        stddev_rate = math.sqrt( sum2/i - mean_rate * mean_rate )\n",
    "        print(f'mean image/s = {mean_rate:8.2f}   standard deviation: {stddev_rate:8.2f}')\n",
    "        tf.profiler.experimental.stop()\n",
    "        sys.exit(0)\n",
    "\n",
    "    # Save the network after every epoch:\n",
    "    checkpoint.save(\"resnet34/model\")\n",
    "    \n",
    "    # Compute the validation accuracy:\n",
    "    mean_accuracy = None\n",
    "    for val_images, val_labels in val_ds.take(steps_validation):\n",
    "        logits = network(val_images)\n",
    "        accuracy = calculate_accuracy(logits, val_labels)\n",
    "        if mean_accuracy is None:\n",
    "            mean_accuracy = accuracy\n",
    "        else:\n",
    "            mean_accuracy += accuracy\n",
    "\n",
    "    mean_accuracy /= steps_validation\n",
    "\n",
    "    print(f\"Validation accuracy after epoch {i_epoch.numpy()}: {mean_accuracy:.4f}.\")\n",
    "\n",
    "\n",
    "# @trace.trace_wrapper('prepare_data_loader')\n",
    "def prepare_data_loader(BATCH_SIZE):\n",
    "\n",
    "    tf.config.threading.set_inter_op_parallelism_threads(parallel_threads)\n",
    "    tf.config.threading.set_intra_op_parallelism_threads(parallel_threads)\n",
    "    print('threading set: ',tf.config.threading.get_inter_op_parallelism_threads(),tf.config.threading.get_intra_op_parallelism_threads())\n",
    "\n",
    "    print(\"Parameters set, preparing dataloading\")\n",
    "    #########################################################################\n",
    "    # Here's the part where we load datasets:\n",
    "    import json\n",
    "\n",
    "\n",
    "    # What's in this function?  Tune in next week ...\n",
    "    from ilsvrc_dataset import get_datasets\n",
    "\n",
    "\n",
    "    class FakeHvd:\n",
    "\n",
    "        def size(self): return 1\n",
    "\n",
    "        def rank(self): return 0\n",
    "\n",
    "\n",
    "    with open(\"ilsvrc.json\", 'r') as f:\n",
    "        config = json.load(f)\n",
    "\n",
    "    config['data']['batch_size'] = BATCH_SIZE\n",
    "    config['data']['num_parallel_readers'] = num_parallel_readers\n",
    "    config['data']['prefetch_buffer_size'] = prefetch_buffer_size \n",
    "\n",
    "    print(json.dumps(config, indent=4))\n",
    "\n",
    "    config['hvd'] = FakeHvd()\n",
    "\n",
    "    train_ds, val_ds = get_datasets(config)\n",
    "\n",
    "    options = tf.data.Options()\n",
    "    options.threading.private_threadpool_size = parallel_threads\n",
    "    train_ds = train_ds.with_options(options)\n",
    "    val_ds = val_ds.with_options(options)\n",
    "\n",
    "    print(\"Datasets ready, creating network.\")\n",
    "    #########################################################################\n",
    "\n",
    "    return train_ds, val_ds\n",
    "\n",
    "\n",
    "def main():\n",
    "    #########################################################################\n",
    "    # Here's some configuration:\n",
    "    #########################################################################\n",
    "    BATCH_SIZE = 256\n",
    "    N_EPOCHS = 10\n",
    "\n",
    "    train_ds, val_ds = prepare_data_loader(BATCH_SIZE)\n",
    "\n",
    "\n",
    "    example_images, example_labels = next(iter(train_ds.take(1)))\n",
    "\n",
    "\n",
    "    print(\"Initial Image size: \", example_images.shape)\n",
    "    network = ResNet34()\n",
    "\n",
    "    output = network(example_images)\n",
    "    print(\"output shape:\", output.shape)\n",
    "\n",
    "    print(network.summary())\n",
    "\n",
    "    epoch = tf.Variable(initial_value=tf.constant(0, dtype=tf.dtypes.int64), name='epoch')\n",
    "    step_in_epoch = tf.Variable(\n",
    "        initial_value=tf.constant(0, dtype=tf.dtypes.int64),\n",
    "        name='step_in_epoch')\n",
    "\n",
    "\n",
    "    # We need an optimizer.  Let's use Adam:\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "\n",
    "    checkpoint = tf.train.Checkpoint(\n",
    "        network       = network,\n",
    "        optimizer     = optimizer,\n",
    "        epoch         = epoch,\n",
    "        step_in_epoch = step_in_epoch)\n",
    "\n",
    "    # Restore the model, if possible:\n",
    "    latest_checkpoint = tf.train.latest_checkpoint(\"resnet34/\")\n",
    "    if latest_checkpoint:\n",
    "        checkpoint.restore(latest_checkpoint)\n",
    "\n",
    "    while epoch < N_EPOCHS:\n",
    "        train_epoch(epoch, step_in_epoch, train_ds, val_ds, network, optimizer, BATCH_SIZE, checkpoint)\n",
    "        epoch.assign_add(1)\n",
    "        step_in_epoch.assign(0)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
